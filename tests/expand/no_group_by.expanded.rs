pub struct QueryStat {
    pub database_id: i64,
    pub calls: i64,
    pub total_time: f64,
}
/// Generated by pco_store to store and load compressed versions of [QueryStat]
pub struct CompressedQueryStats {
    database_id: Vec<u8>,
    calls: Vec<u8>,
    total_time: Vec<u8>,
}
impl CompressedQueryStats {
    /// Loads data for the specified filters.
    ///
    /// For models with a timestamp, [decompress][Self::decompress] automatically filters out
    /// rows outside the requested time range.
    pub async fn load(
        db: &deadpool_postgres::Object,
    ) -> anyhow::Result<Vec<CompressedQueryStats>> {
        let sql = "SELECT * FROM query_stats WHERE true";
        let mut results = Vec::new();
        for row in db.query(&db.prepare_cached(&sql).await?, &[]).await? {
            results
                .push(CompressedQueryStats {
                    database_id: row.get(0usize),
                    calls: row.get(1usize),
                    total_time: row.get(2usize),
                });
        }
        Ok(results)
    }
    /// Deletes data for the specified filters, returning it to the caller.
    ///
    /// For models with a timestamp, [decompress][Self::decompress] **will not** filter out
    /// rows outside the requested time range.
    pub async fn delete(
        db: &deadpool_postgres::Object,
    ) -> anyhow::Result<Vec<CompressedQueryStats>> {
        let sql = "DELETE FROM query_stats WHERE true RETURNING *";
        let mut results = Vec::new();
        for row in db.query(&db.prepare_cached(&sql).await?, &[]).await? {
            results
                .push(CompressedQueryStats {
                    database_id: row.get(0usize),
                    calls: row.get(1usize),
                    total_time: row.get(2usize),
                });
        }
        Ok(results)
    }
    /// Decompresses a group of data points.
    pub fn decompress(self) -> anyhow::Result<Vec<QueryStat>> {
        let mut results = Vec::new();
        let database_id: Vec<i64> = if self.database_id.is_empty() {
            Vec::new()
        } else {
            ::pco::standalone::simple_decompress(&self.database_id)?
        };
        let calls: Vec<i64> = if self.calls.is_empty() {
            Vec::new()
        } else {
            ::pco::standalone::simple_decompress(&self.calls)?
        };
        let total_time: Vec<f64> = if self.total_time.is_empty() {
            Vec::new()
        } else {
            ::pco::standalone::simple_decompress(&self.total_time)?
        };
        let len = [database_id.len(), calls.len(), total_time.len()]
            .into_iter()
            .max()
            .unwrap_or(0);
        for index in 0..len {
            let row = QueryStat {
                database_id: database_id.get(index).cloned().unwrap_or_default(),
                calls: calls.get(index).cloned().unwrap_or_default(),
                total_time: total_time.get(index).cloned().unwrap_or_default(),
            };
            if true {
                results.push(row);
            }
        }
        Ok(results)
    }
    /// Writes the data to disk.
    pub async fn store(
        db: &deadpool_postgres::Object,
        rows: Vec<QueryStat>,
    ) -> anyhow::Result<()> {
        if rows.is_empty() {
            return Ok(());
        }
        let mut grouped_rows: ahash::AHashMap<_, Vec<QueryStat>> = ahash::AHashMap::new();
        for row in rows {
            grouped_rows.entry(()).or_default().push(row);
        }
        let sql = "COPY query_stats (database_id, calls, total_time) FROM STDIN BINARY";
        let types = &[
            tokio_postgres::types::Type::BYTEA,
            tokio_postgres::types::Type::BYTEA,
            tokio_postgres::types::Type::BYTEA,
        ];
        let stmt = db.copy_in(&db.prepare_cached(&sql).await?).await?;
        let writer = tokio_postgres::binary_copy::BinaryCopyInWriter::new(stmt, types);
        let mut writer = writer;
        #[allow(unused_mut)]
        let mut writer = unsafe {
            ::pin_utils::core_reexport::pin::Pin::new_unchecked(&mut writer)
        };
        for rows in grouped_rows.into_values() {
            writer
                .as_mut()
                .write(
                    &[
                        &::pco::standalone::simpler_compress(
                                &rows.iter().map(|r| r.database_id).collect::<Vec<_>>(),
                                ::pco::DEFAULT_COMPRESSION_LEVEL,
                            )
                            .unwrap(),
                        &::pco::standalone::simpler_compress(
                                &rows.iter().map(|r| r.calls).collect::<Vec<_>>(),
                                ::pco::DEFAULT_COMPRESSION_LEVEL,
                            )
                            .unwrap(),
                        &::pco::standalone::simpler_compress(
                                &rows.iter().map(|r| r.total_time).collect::<Vec<_>>(),
                                ::pco::DEFAULT_COMPRESSION_LEVEL,
                            )
                            .unwrap(),
                    ],
                )
                .await?;
        }
        writer.finish().await?;
        Ok(())
    }
    /// Writes the data to disk, with the provided grouping closure applied.
    ///
    /// This can be used to improve the compression ratio and reduce read IO, for example
    /// by compacting real-time data into a single row per hour / day / week.
    pub async fn store_grouped<F, R>(
        db: &deadpool_postgres::Object,
        rows: Vec<QueryStat>,
        grouping: F,
    ) -> anyhow::Result<()>
    where
        F: Fn(&QueryStat) -> R,
        R: Eq + std::hash::Hash,
    {
        if rows.is_empty() {
            return Ok(());
        }
        let mut grouped_rows: ahash::AHashMap<_, Vec<QueryStat>> = ahash::AHashMap::new();
        for row in rows {
            grouped_rows.entry((grouping(&row))).or_default().push(row);
        }
        let sql = "COPY query_stats (database_id, calls, total_time) FROM STDIN BINARY";
        let types = &[
            tokio_postgres::types::Type::BYTEA,
            tokio_postgres::types::Type::BYTEA,
            tokio_postgres::types::Type::BYTEA,
        ];
        let stmt = db.copy_in(&db.prepare_cached(&sql).await?).await?;
        let writer = tokio_postgres::binary_copy::BinaryCopyInWriter::new(stmt, types);
        let mut writer = writer;
        #[allow(unused_mut)]
        let mut writer = unsafe {
            ::pin_utils::core_reexport::pin::Pin::new_unchecked(&mut writer)
        };
        for rows in grouped_rows.into_values() {
            writer
                .as_mut()
                .write(
                    &[
                        &::pco::standalone::simpler_compress(
                                &rows.iter().map(|r| r.database_id).collect::<Vec<_>>(),
                                ::pco::DEFAULT_COMPRESSION_LEVEL,
                            )
                            .unwrap(),
                        &::pco::standalone::simpler_compress(
                                &rows.iter().map(|r| r.calls).collect::<Vec<_>>(),
                                ::pco::DEFAULT_COMPRESSION_LEVEL,
                            )
                            .unwrap(),
                        &::pco::standalone::simpler_compress(
                                &rows.iter().map(|r| r.total_time).collect::<Vec<_>>(),
                                ::pco::DEFAULT_COMPRESSION_LEVEL,
                            )
                            .unwrap(),
                    ],
                )
                .await?;
        }
        writer.finish().await?;
        Ok(())
    }
}
